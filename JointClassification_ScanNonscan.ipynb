{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73b7f1c6",
   "metadata": {},
   "source": [
    "0. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87ec18b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # Import PyTorch for deep learning computations\n",
    "import torchvision  # Import torchvision for pre-trained models and datasets\n",
    "import torch.nn as nn  # Import neural network module from PyTorch\n",
    "import torch.optim as optim  # Import optimizers for training models\n",
    "import time  # Import time module for measuring execution time\n",
    "import numpy as np  # Import NumPy for numerical operations\n",
    "import matplotlib.pyplot as plt  # Import Matplotlib for visualization\n",
    "import os  # Import os for file system operations\n",
    "import zipfile  # Import zipfile for extracting compressed datasets\n",
    "import requests  # Import requests for downloading files\n",
    "\n",
    "import pandas as pd  # Import pandas for data manipulation and analysis\n",
    "from PIL import Image  # Import PIL for image processing\n",
    "from torchvision import datasets, models, transforms  # Import datasets, pre-trained models, and transformations from torchvision\n",
    "from torchinfo import summary  # Import torchinfo for displaying model summaries\n",
    "from torch.utils.data import DataLoader  # Import DataLoader for handling batch data loading\n",
    "\n",
    "plt.style.use('ggplot')  # Set the Matplotlib style to 'ggplot' for better visuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a593c12f",
   "metadata": {},
   "source": [
    "1. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0af9b42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining transformations to be applied to training, validation, and test datasets\n",
    "image_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),  # Randomly crop the image with scaling\n",
    "        transforms.RandomRotation(degrees=15),  # Apply random rotation up to 15 degrees\n",
    "        transforms.RandomHorizontalFlip(),  # Flip the image horizontally with a probability of 0.5\n",
    "        transforms.CenterCrop(size=224),  # Crop the center of the image to 224x224 pixels\n",
    "        transforms.ToTensor(),  # Convert image to PyTorch tensor format\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],  # Normalize image using mean\n",
    "                             [0.229, 0.224, 0.225])  # Normalize image using std deviation\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(size=256),  # Resize the image to 256 pixels on the shorter side\n",
    "        transforms.CenterCrop(size=224),  # Crop the center of the image to 224x224 pixels\n",
    "        transforms.ToTensor(),  # Convert image to PyTorch tensor format\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],  # Normalize image using mean\n",
    "                             [0.229, 0.224, 0.225])  # Normalize image using std deviation\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(size=256),  # Resize the image to 256 pixels on the shorter side\n",
    "        transforms.CenterCrop(size=224),  # Crop the center of the image to 224x224 pixels\n",
    "        transforms.ToTensor(),  # Convert image to PyTorch tensor format\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],  # Normalize image using mean\n",
    "                             [0.229, 0.224, 0.225])  # Normalize image using std deviation\n",
    "    ])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c89a2ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "{0: 'nonscan', 1: 'scan'}\n"
     ]
    }
   ],
   "source": [
    "# Load the Data\n",
    "\n",
    "# Set train and valid directory paths\n",
    "\n",
    "dataset = 'JointDetection_ScanNonscan'\n",
    "\n",
    "train_directory = os.path.join('data',dataset, 'train')\n",
    "valid_directory = os.path.join('data',dataset, 'valid')\n",
    "test_directory = os.path.join('data',dataset, 'test')\n",
    "\n",
    "# Batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Number of classes\n",
    "num_classes = len(os.listdir(valid_directory))  #10#2#257\n",
    "print(num_classes)\n",
    "\n",
    "# Load Data from folders\n",
    "data = {\n",
    "    'train': datasets.ImageFolder(root=train_directory, transform=image_transforms['train']),\n",
    "    'valid': datasets.ImageFolder(root=valid_directory, transform=image_transforms['valid']),\n",
    "    'test': datasets.ImageFolder(root=test_directory, transform=image_transforms['test'])\n",
    "}\n",
    "\n",
    "# Get a mapping of the indices to the class names, in order to see the output classes of the test images.\n",
    "idx_to_class = {v: k for k, v in data['train'].class_to_idx.items()}\n",
    "print(idx_to_class)\n",
    "\n",
    "# Size of Data, to be used for calculating Average Loss and Accuracy\n",
    "train_data_size = len(data['train'])\n",
    "valid_data_size = len(data['valid'])\n",
    "test_data_size = len(data['test'])\n",
    "\n",
    "# Create iterators for the Data loaded using DataLoader module\n",
    "train_data_loader = DataLoader(data['train'], batch_size=batch_size, shuffle=True)\n",
    "valid_data_loader = DataLoader(data['valid'], batch_size=batch_size, shuffle=False)\n",
    "test_data_loader = DataLoader(data['test'], batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4b10828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:   1719\n",
      "Number of validation samples: 318\n",
      "Number of test samples:       287\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Number of training samples:   {train_data_size}\")\n",
    "print(f\"Number of validation samples: {valid_data_size}\"),\n",
    "print(f\"Number of test samples:       {test_data_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d62066",
   "metadata": {},
   "source": [
    "2. Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d63254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained ResNet50 Model\n",
    "resnet50 = models.resnet50(weights='DEFAULT')\n",
    "resnet50 = resnet50.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e9cad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze model parameters\n",
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa571bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the final fully connected layer of the ResNet50 model for transfer learning\n",
    "fc_inputs = resnet50.fc.in_features  # Get the number of input features for the final layer\n",
    "\n",
    "# Define a new fully connected layer with custom architecture for classification\n",
    "resnet50.fc = nn.Sequential(\n",
    "    nn.Linear(fc_inputs, 256),  # Fully connected layer with 256 neurons\n",
    "    nn.ReLU(),  # Apply ReLU activation\n",
    "    nn.Dropout(0.4),  # Apply dropout with 40% probability to prevent overfitting\n",
    "    nn.Linear(256, num_classes),  # Output layer with number of classes as output neurons\n",
    "    nn.LogSoftmax(dim=1)  # Apply LogSoftmax for multi-class classification (used with NLLLoss)\n",
    ")\n",
    "\n",
    "# Move the model to the appropriate device (either CUDA or CPU)\n",
    "resnet50 = resnet50.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abb94586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function for classification\n",
    "loss_func = nn.NLLLoss()  # NLLLoss is suitable for multi-class classification\n",
    "\n",
    "# Define the learning rate for the optimizer\n",
    "learning_rate = 0.01  # Initial learning rate for the optimizer\n",
    "\n",
    "# Define the optimizer using Stochastic Gradient Descent (SGD)\n",
    "optimizer = optim.SGD(\n",
    "    params=resnet50.parameters(),  # Optimizing all parameters of the ResNet50 model\n",
    "    lr=learning_rate,  # Learning rate value\n",
    "    momentum=0.9  # Momentum term to improve convergence and avoid local minima\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6cb76f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc.0.weight: requires_grad = True\n",
      "fc.0.bias: requires_grad = True\n",
      "fc.3.weight: requires_grad = True\n",
      "fc.3.bias: requires_grad = True\n"
     ]
    }
   ],
   "source": [
    "for name, param in resnet50.named_parameters():\n",
    "    if 'fc' in name:\n",
    "        print(f\"{name}: requires_grad = {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cfe130",
   "metadata": {},
   "source": [
    "3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65e3708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n",
    "    \"\"\"\n",
    "    Function to train and validate\n",
    "    Parameters\n",
    "        :param model: Model to train and validate\n",
    "        :param loss_criterion: Loss Criterion to minimize\n",
    "        :param optimizer: Optimizer for computing gradients\n",
    "        :param epochs: Number of epochs (default=25)\n",
    "\n",
    "    Returns\n",
    "        model: Trained Model with best validation accuracy\n",
    "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    history = []\n",
    "    best_loss = 100000.0\n",
    "    best_epoch = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
    "\n",
    "        # Set to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Loss and Accuracy within the epoch\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "\n",
    "        valid_loss = 0.0\n",
    "        valid_acc = 0.0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_data_loader):\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Clean existing gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass - compute outputs on input data using the model\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_criterion(outputs, labels)\n",
    "\n",
    "            # Backpropagate the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute the total loss for the batch and add it to train_loss\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Compute the accuracy\n",
    "            ret, predictions = torch.max(outputs.data, 1)\n",
    "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "\n",
    "            # Convert correct_counts to float and then compute the mean\n",
    "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "            # Compute total accuracy in the whole batch and add to train_acc\n",
    "            train_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "            #print(\"Batch number: {:03d}, Training Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
    "\n",
    "\n",
    "        # Validation - No gradient tracking needed\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Set to evaluation mode\n",
    "            model.eval()\n",
    "\n",
    "            # Validation loop\n",
    "            for j, (inputs, labels) in enumerate(valid_data_loader):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass - compute outputs on input data using the model\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = loss_criterion(outputs, labels)\n",
    "\n",
    "                # Compute the total loss for the batch and add it to valid_loss\n",
    "                valid_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                # Calculate validation accuracy\n",
    "                ret, predictions = torch.max(outputs.data, 1)\n",
    "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "\n",
    "                # Convert correct_counts to float and then compute the mean\n",
    "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "                # Compute total accuracy in the whole batch and add to valid_acc\n",
    "                valid_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            best_epoch = epoch\n",
    "            # Save if the model has best accuracy till now\n",
    "            torch.save(model, 'best_model.pt')\n",
    "\n",
    "        # Find average training loss and training accuracy\n",
    "        avg_train_loss = train_loss/train_data_size\n",
    "        avg_train_acc = train_acc/train_data_size\n",
    "\n",
    "        # Find average training loss and training accuracy\n",
    "        avg_valid_loss = valid_loss/valid_data_size\n",
    "        avg_valid_acc = valid_acc/valid_data_size\n",
    "\n",
    "        history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
    "\n",
    "        epoch_end = time.time()\n",
    "\n",
    "        print(\"Epoch : {:03d}, Training: Loss - {:.4f}, Accuracy - {:.4f}%, \\n\\t\\tValidation : Loss - {:.4f}, Accuracy - {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return model, history, best_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dda56a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "ResNet                                   [32, 2]                   --\n",
      "├─Conv2d: 1-1                            [32, 64, 112, 112]        (9,408)\n",
      "├─BatchNorm2d: 1-2                       [32, 64, 112, 112]        (128)\n",
      "├─ReLU: 1-3                              [32, 64, 112, 112]        --\n",
      "├─MaxPool2d: 1-4                         [32, 64, 56, 56]          --\n",
      "├─Sequential: 1-5                        [32, 256, 56, 56]         --\n",
      "│    └─Bottleneck: 2-1                   [32, 256, 56, 56]         --\n",
      "│    │    └─Conv2d: 3-1                  [32, 64, 56, 56]          (4,096)\n",
      "│    │    └─BatchNorm2d: 3-2             [32, 64, 56, 56]          (128)\n",
      "│    │    └─ReLU: 3-3                    [32, 64, 56, 56]          --\n",
      "│    │    └─Conv2d: 3-4                  [32, 64, 56, 56]          (36,864)\n",
      "│    │    └─BatchNorm2d: 3-5             [32, 64, 56, 56]          (128)\n",
      "│    │    └─ReLU: 3-6                    [32, 64, 56, 56]          --\n",
      "│    │    └─Conv2d: 3-7                  [32, 256, 56, 56]         (16,384)\n",
      "│    │    └─BatchNorm2d: 3-8             [32, 256, 56, 56]         (512)\n",
      "│    │    └─Sequential: 3-9              [32, 256, 56, 56]         (16,896)\n",
      "│    │    └─ReLU: 3-10                   [32, 256, 56, 56]         --\n",
      "│    └─Bottleneck: 2-2                   [32, 256, 56, 56]         --\n",
      "│    │    └─Conv2d: 3-11                 [32, 64, 56, 56]          (16,384)\n",
      "│    │    └─BatchNorm2d: 3-12            [32, 64, 56, 56]          (128)\n",
      "│    │    └─ReLU: 3-13                   [32, 64, 56, 56]          --\n",
      "│    │    └─Conv2d: 3-14                 [32, 64, 56, 56]          (36,864)\n",
      "│    │    └─BatchNorm2d: 3-15            [32, 64, 56, 56]          (128)\n",
      "│    │    └─ReLU: 3-16                   [32, 64, 56, 56]          --\n",
      "│    │    └─Conv2d: 3-17                 [32, 256, 56, 56]         (16,384)\n",
      "│    │    └─BatchNorm2d: 3-18            [32, 256, 56, 56]         (512)\n",
      "│    │    └─ReLU: 3-19                   [32, 256, 56, 56]         --\n",
      "│    └─Bottleneck: 2-3                   [32, 256, 56, 56]         --\n",
      "│    │    └─Conv2d: 3-20                 [32, 64, 56, 56]          (16,384)\n",
      "│    │    └─BatchNorm2d: 3-21            [32, 64, 56, 56]          (128)\n",
      "│    │    └─ReLU: 3-22                   [32, 64, 56, 56]          --\n",
      "│    │    └─Conv2d: 3-23                 [32, 64, 56, 56]          (36,864)\n",
      "│    │    └─BatchNorm2d: 3-24            [32, 64, 56, 56]          (128)\n",
      "│    │    └─ReLU: 3-25                   [32, 64, 56, 56]          --\n",
      "│    │    └─Conv2d: 3-26                 [32, 256, 56, 56]         (16,384)\n",
      "│    │    └─BatchNorm2d: 3-27            [32, 256, 56, 56]         (512)\n",
      "│    │    └─ReLU: 3-28                   [32, 256, 56, 56]         --\n",
      "├─Sequential: 1-6                        [32, 512, 28, 28]         --\n",
      "│    └─Bottleneck: 2-4                   [32, 512, 28, 28]         --\n",
      "│    │    └─Conv2d: 3-29                 [32, 128, 56, 56]         (32,768)\n",
      "│    │    └─BatchNorm2d: 3-30            [32, 128, 56, 56]         (256)\n",
      "│    │    └─ReLU: 3-31                   [32, 128, 56, 56]         --\n",
      "│    │    └─Conv2d: 3-32                 [32, 128, 28, 28]         (147,456)\n",
      "│    │    └─BatchNorm2d: 3-33            [32, 128, 28, 28]         (256)\n",
      "│    │    └─ReLU: 3-34                   [32, 128, 28, 28]         --\n",
      "│    │    └─Conv2d: 3-35                 [32, 512, 28, 28]         (65,536)\n",
      "│    │    └─BatchNorm2d: 3-36            [32, 512, 28, 28]         (1,024)\n",
      "│    │    └─Sequential: 3-37             [32, 512, 28, 28]         (132,096)\n",
      "│    │    └─ReLU: 3-38                   [32, 512, 28, 28]         --\n",
      "│    └─Bottleneck: 2-5                   [32, 512, 28, 28]         --\n",
      "│    │    └─Conv2d: 3-39                 [32, 128, 28, 28]         (65,536)\n",
      "│    │    └─BatchNorm2d: 3-40            [32, 128, 28, 28]         (256)\n",
      "│    │    └─ReLU: 3-41                   [32, 128, 28, 28]         --\n",
      "│    │    └─Conv2d: 3-42                 [32, 128, 28, 28]         (147,456)\n",
      "│    │    └─BatchNorm2d: 3-43            [32, 128, 28, 28]         (256)\n",
      "│    │    └─ReLU: 3-44                   [32, 128, 28, 28]         --\n",
      "│    │    └─Conv2d: 3-45                 [32, 512, 28, 28]         (65,536)\n",
      "│    │    └─BatchNorm2d: 3-46            [32, 512, 28, 28]         (1,024)\n",
      "│    │    └─ReLU: 3-47                   [32, 512, 28, 28]         --\n",
      "│    └─Bottleneck: 2-6                   [32, 512, 28, 28]         --\n",
      "│    │    └─Conv2d: 3-48                 [32, 128, 28, 28]         (65,536)\n",
      "│    │    └─BatchNorm2d: 3-49            [32, 128, 28, 28]         (256)\n",
      "│    │    └─ReLU: 3-50                   [32, 128, 28, 28]         --\n",
      "│    │    └─Conv2d: 3-51                 [32, 128, 28, 28]         (147,456)\n",
      "│    │    └─BatchNorm2d: 3-52            [32, 128, 28, 28]         (256)\n",
      "│    │    └─ReLU: 3-53                   [32, 128, 28, 28]         --\n",
      "│    │    └─Conv2d: 3-54                 [32, 512, 28, 28]         (65,536)\n",
      "│    │    └─BatchNorm2d: 3-55            [32, 512, 28, 28]         (1,024)\n",
      "│    │    └─ReLU: 3-56                   [32, 512, 28, 28]         --\n",
      "│    └─Bottleneck: 2-7                   [32, 512, 28, 28]         --\n",
      "│    │    └─Conv2d: 3-57                 [32, 128, 28, 28]         (65,536)\n",
      "│    │    └─BatchNorm2d: 3-58            [32, 128, 28, 28]         (256)\n",
      "│    │    └─ReLU: 3-59                   [32, 128, 28, 28]         --\n",
      "│    │    └─Conv2d: 3-60                 [32, 128, 28, 28]         (147,456)\n",
      "│    │    └─BatchNorm2d: 3-61            [32, 128, 28, 28]         (256)\n",
      "│    │    └─ReLU: 3-62                   [32, 128, 28, 28]         --\n",
      "│    │    └─Conv2d: 3-63                 [32, 512, 28, 28]         (65,536)\n",
      "│    │    └─BatchNorm2d: 3-64            [32, 512, 28, 28]         (1,024)\n",
      "│    │    └─ReLU: 3-65                   [32, 512, 28, 28]         --\n",
      "├─Sequential: 1-7                        [32, 1024, 14, 14]        --\n",
      "│    └─Bottleneck: 2-8                   [32, 1024, 14, 14]        --\n",
      "│    │    └─Conv2d: 3-66                 [32, 256, 28, 28]         (131,072)\n",
      "│    │    └─BatchNorm2d: 3-67            [32, 256, 28, 28]         (512)\n",
      "│    │    └─ReLU: 3-68                   [32, 256, 28, 28]         --\n",
      "│    │    └─Conv2d: 3-69                 [32, 256, 14, 14]         (589,824)\n",
      "│    │    └─BatchNorm2d: 3-70            [32, 256, 14, 14]         (512)\n",
      "│    │    └─ReLU: 3-71                   [32, 256, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-72                 [32, 1024, 14, 14]        (262,144)\n",
      "│    │    └─BatchNorm2d: 3-73            [32, 1024, 14, 14]        (2,048)\n",
      "│    │    └─Sequential: 3-74             [32, 1024, 14, 14]        (526,336)\n",
      "│    │    └─ReLU: 3-75                   [32, 1024, 14, 14]        --\n",
      "│    └─Bottleneck: 2-9                   [32, 1024, 14, 14]        --\n",
      "│    │    └─Conv2d: 3-76                 [32, 256, 14, 14]         (262,144)\n",
      "│    │    └─BatchNorm2d: 3-77            [32, 256, 14, 14]         (512)\n",
      "│    │    └─ReLU: 3-78                   [32, 256, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-79                 [32, 256, 14, 14]         (589,824)\n",
      "│    │    └─BatchNorm2d: 3-80            [32, 256, 14, 14]         (512)\n",
      "│    │    └─ReLU: 3-81                   [32, 256, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-82                 [32, 1024, 14, 14]        (262,144)\n",
      "│    │    └─BatchNorm2d: 3-83            [32, 1024, 14, 14]        (2,048)\n",
      "│    │    └─ReLU: 3-84                   [32, 1024, 14, 14]        --\n",
      "│    └─Bottleneck: 2-10                  [32, 1024, 14, 14]        --\n",
      "│    │    └─Conv2d: 3-85                 [32, 256, 14, 14]         (262,144)\n",
      "│    │    └─BatchNorm2d: 3-86            [32, 256, 14, 14]         (512)\n",
      "│    │    └─ReLU: 3-87                   [32, 256, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-88                 [32, 256, 14, 14]         (589,824)\n",
      "│    │    └─BatchNorm2d: 3-89            [32, 256, 14, 14]         (512)\n",
      "│    │    └─ReLU: 3-90                   [32, 256, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-91                 [32, 1024, 14, 14]        (262,144)\n",
      "│    │    └─BatchNorm2d: 3-92            [32, 1024, 14, 14]        (2,048)\n",
      "│    │    └─ReLU: 3-93                   [32, 1024, 14, 14]        --\n",
      "│    └─Bottleneck: 2-11                  [32, 1024, 14, 14]        --\n",
      "│    │    └─Conv2d: 3-94                 [32, 256, 14, 14]         (262,144)\n",
      "│    │    └─BatchNorm2d: 3-95            [32, 256, 14, 14]         (512)\n",
      "│    │    └─ReLU: 3-96                   [32, 256, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-97                 [32, 256, 14, 14]         (589,824)\n",
      "│    │    └─BatchNorm2d: 3-98            [32, 256, 14, 14]         (512)\n",
      "│    │    └─ReLU: 3-99                   [32, 256, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-100                [32, 1024, 14, 14]        (262,144)\n",
      "│    │    └─BatchNorm2d: 3-101           [32, 1024, 14, 14]        (2,048)\n",
      "│    │    └─ReLU: 3-102                  [32, 1024, 14, 14]        --\n",
      "│    └─Bottleneck: 2-12                  [32, 1024, 14, 14]        --\n",
      "│    │    └─Conv2d: 3-103                [32, 256, 14, 14]         (262,144)\n",
      "│    │    └─BatchNorm2d: 3-104           [32, 256, 14, 14]         (512)\n",
      "│    │    └─ReLU: 3-105                  [32, 256, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-106                [32, 256, 14, 14]         (589,824)\n",
      "│    │    └─BatchNorm2d: 3-107           [32, 256, 14, 14]         (512)\n",
      "│    │    └─ReLU: 3-108                  [32, 256, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-109                [32, 1024, 14, 14]        (262,144)\n",
      "│    │    └─BatchNorm2d: 3-110           [32, 1024, 14, 14]        (2,048)\n",
      "│    │    └─ReLU: 3-111                  [32, 1024, 14, 14]        --\n",
      "│    └─Bottleneck: 2-13                  [32, 1024, 14, 14]        --\n",
      "│    │    └─Conv2d: 3-112                [32, 256, 14, 14]         (262,144)\n",
      "│    │    └─BatchNorm2d: 3-113           [32, 256, 14, 14]         (512)\n",
      "│    │    └─ReLU: 3-114                  [32, 256, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-115                [32, 256, 14, 14]         (589,824)\n",
      "│    │    └─BatchNorm2d: 3-116           [32, 256, 14, 14]         (512)\n",
      "│    │    └─ReLU: 3-117                  [32, 256, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-118                [32, 1024, 14, 14]        (262,144)\n",
      "│    │    └─BatchNorm2d: 3-119           [32, 1024, 14, 14]        (2,048)\n",
      "│    │    └─ReLU: 3-120                  [32, 1024, 14, 14]        --\n",
      "├─Sequential: 1-8                        [32, 2048, 7, 7]          --\n",
      "│    └─Bottleneck: 2-14                  [32, 2048, 7, 7]          --\n",
      "│    │    └─Conv2d: 3-121                [32, 512, 14, 14]         (524,288)\n",
      "│    │    └─BatchNorm2d: 3-122           [32, 512, 14, 14]         (1,024)\n",
      "│    │    └─ReLU: 3-123                  [32, 512, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-124                [32, 512, 7, 7]           (2,359,296)\n",
      "│    │    └─BatchNorm2d: 3-125           [32, 512, 7, 7]           (1,024)\n",
      "│    │    └─ReLU: 3-126                  [32, 512, 7, 7]           --\n",
      "│    │    └─Conv2d: 3-127                [32, 2048, 7, 7]          (1,048,576)\n",
      "│    │    └─BatchNorm2d: 3-128           [32, 2048, 7, 7]          (4,096)\n",
      "│    │    └─Sequential: 3-129            [32, 2048, 7, 7]          (2,101,248)\n",
      "│    │    └─ReLU: 3-130                  [32, 2048, 7, 7]          --\n",
      "│    └─Bottleneck: 2-15                  [32, 2048, 7, 7]          --\n",
      "│    │    └─Conv2d: 3-131                [32, 512, 7, 7]           (1,048,576)\n",
      "│    │    └─BatchNorm2d: 3-132           [32, 512, 7, 7]           (1,024)\n",
      "│    │    └─ReLU: 3-133                  [32, 512, 7, 7]           --\n",
      "│    │    └─Conv2d: 3-134                [32, 512, 7, 7]           (2,359,296)\n",
      "│    │    └─BatchNorm2d: 3-135           [32, 512, 7, 7]           (1,024)\n",
      "│    │    └─ReLU: 3-136                  [32, 512, 7, 7]           --\n",
      "│    │    └─Conv2d: 3-137                [32, 2048, 7, 7]          (1,048,576)\n",
      "│    │    └─BatchNorm2d: 3-138           [32, 2048, 7, 7]          (4,096)\n",
      "│    │    └─ReLU: 3-139                  [32, 2048, 7, 7]          --\n",
      "│    └─Bottleneck: 2-16                  [32, 2048, 7, 7]          --\n",
      "│    │    └─Conv2d: 3-140                [32, 512, 7, 7]           (1,048,576)\n",
      "│    │    └─BatchNorm2d: 3-141           [32, 512, 7, 7]           (1,024)\n",
      "│    │    └─ReLU: 3-142                  [32, 512, 7, 7]           --\n",
      "│    │    └─Conv2d: 3-143                [32, 512, 7, 7]           (2,359,296)\n",
      "│    │    └─BatchNorm2d: 3-144           [32, 512, 7, 7]           (1,024)\n",
      "│    │    └─ReLU: 3-145                  [32, 512, 7, 7]           --\n",
      "│    │    └─Conv2d: 3-146                [32, 2048, 7, 7]          (1,048,576)\n",
      "│    │    └─BatchNorm2d: 3-147           [32, 2048, 7, 7]          (4,096)\n",
      "│    │    └─ReLU: 3-148                  [32, 2048, 7, 7]          --\n",
      "├─AdaptiveAvgPool2d: 1-9                 [32, 2048, 1, 1]          --\n",
      "├─Sequential: 1-10                       [32, 2]                   --\n",
      "│    └─Linear: 2-17                      [32, 256]                 524,544\n",
      "│    └─ReLU: 2-18                        [32, 256]                 --\n",
      "│    └─Dropout: 2-19                     [32, 256]                 --\n",
      "│    └─Linear: 2-20                      [32, 2]                   514\n",
      "│    └─LogSoftmax: 2-21                  [32, 2]                   --\n",
      "==========================================================================================\n",
      "Total params: 24,033,090\n",
      "Trainable params: 525,058\n",
      "Non-trainable params: 23,508,032\n",
      "Total mult-adds (Units.GIGABYTES): 130.81\n",
      "==========================================================================================\n",
      "Input size (MB): 19.27\n",
      "Forward/backward pass size (MB): 5690.43\n",
      "Params size (MB): 96.13\n",
      "Estimated Total Size (MB): 5805.83\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Print the model to be trained.\n",
    "print(summary(resnet50, input_size=(batch_size, 3, 224, 224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c64bbfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/25\n",
      "Epoch : 000, Training: Loss - 0.4786, Accuracy - 79.7557%, \n",
      "\t\tValidation : Loss - 0.2124, Accuracy - 94.9686%, Time: 32.3992s\n",
      "Epoch: 2/25\n",
      "Epoch : 001, Training: Loss - 0.2420, Accuracy - 91.3903%, \n",
      "\t\tValidation : Loss - 0.1750, Accuracy - 94.6541%, Time: 28.0681s\n",
      "Epoch: 3/25\n",
      "Epoch : 002, Training: Loss - 0.1920, Accuracy - 93.3101%, \n",
      "\t\tValidation : Loss - 0.1457, Accuracy - 95.5975%, Time: 25.6518s\n",
      "Epoch: 4/25\n",
      "Epoch : 003, Training: Loss - 0.1596, Accuracy - 94.4154%, \n",
      "\t\tValidation : Loss - 0.1531, Accuracy - 95.9119%, Time: 24.7154s\n",
      "Epoch: 5/25\n",
      "Epoch : 004, Training: Loss - 0.1553, Accuracy - 94.4735%, \n",
      "\t\tValidation : Loss - 0.1798, Accuracy - 94.6541%, Time: 24.8089s\n",
      "Epoch: 6/25\n",
      "Epoch : 005, Training: Loss - 0.1544, Accuracy - 93.7755%, \n",
      "\t\tValidation : Loss - 0.2449, Accuracy - 90.2516%, Time: 27.1653s\n",
      "Epoch: 7/25\n",
      "Epoch : 006, Training: Loss - 0.1552, Accuracy - 94.9389%, \n",
      "\t\tValidation : Loss - 0.1388, Accuracy - 95.2830%, Time: 26.6510s\n",
      "Epoch: 8/25\n",
      "Epoch : 007, Training: Loss - 0.1215, Accuracy - 96.5096%, \n",
      "\t\tValidation : Loss - 0.1334, Accuracy - 95.5975%, Time: 26.0124s\n",
      "Epoch: 9/25\n",
      "Epoch : 008, Training: Loss - 0.1401, Accuracy - 94.7062%, \n",
      "\t\tValidation : Loss - 0.1318, Accuracy - 95.2830%, Time: 25.9682s\n",
      "Epoch: 10/25\n",
      "Epoch : 009, Training: Loss - 0.1320, Accuracy - 95.1134%, \n",
      "\t\tValidation : Loss - 0.1338, Accuracy - 95.9119%, Time: 28.5084s\n",
      "Epoch: 11/25\n",
      "Epoch : 010, Training: Loss - 0.1226, Accuracy - 95.0553%, \n",
      "\t\tValidation : Loss - 0.1238, Accuracy - 95.5975%, Time: 26.1637s\n",
      "Epoch: 12/25\n",
      "Epoch : 011, Training: Loss - 0.1046, Accuracy - 96.0442%, \n",
      "\t\tValidation : Loss - 0.1268, Accuracy - 95.9119%, Time: 31.3919s\n",
      "Epoch: 13/25\n",
      "Epoch : 012, Training: Loss - 0.1194, Accuracy - 95.9860%, \n",
      "\t\tValidation : Loss - 0.1220, Accuracy - 96.5409%, Time: 26.7167s\n",
      "Epoch: 14/25\n",
      "Epoch : 013, Training: Loss - 0.1042, Accuracy - 95.6952%, \n",
      "\t\tValidation : Loss - 0.1665, Accuracy - 93.3962%, Time: 26.2957s\n",
      "Epoch: 15/25\n",
      "Epoch : 014, Training: Loss - 0.1163, Accuracy - 95.4043%, \n",
      "\t\tValidation : Loss - 0.1230, Accuracy - 96.5409%, Time: 29.4010s\n",
      "Epoch: 16/25\n",
      "Epoch : 015, Training: Loss - 0.1227, Accuracy - 95.6952%, \n",
      "\t\tValidation : Loss - 0.1180, Accuracy - 96.8553%, Time: 31.4055s\n",
      "Epoch: 17/25\n",
      "Epoch : 016, Training: Loss - 0.0952, Accuracy - 96.3351%, \n",
      "\t\tValidation : Loss - 0.1354, Accuracy - 94.6541%, Time: 29.6913s\n",
      "Epoch: 18/25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train the model.\u001b[39;00m\n\u001b[32m      2\u001b[39m num_epochs = \u001b[32m25\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m trained_model, history, best_epoch = \u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresnet50\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m torch.save(history, dataset+\u001b[33m'\u001b[39m\u001b[33m_history.pt\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mtrain_and_validate\u001b[39m\u001b[34m(model, loss_criterion, optimizer, epochs)\u001b[39m\n\u001b[32m     31\u001b[39m valid_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m     32\u001b[39m valid_acc = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/opencv-env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/opencv-env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/opencv-env/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/opencv-env/lib/python3.13/site-packages/torchvision/datasets/folder.py:247\u001b[39m, in \u001b[36mDatasetFolder.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    245\u001b[39m sample = \u001b[38;5;28mself\u001b[39m.loader(path)\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     sample = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    249\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/opencv-env/lib/python3.13/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/opencv-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/opencv-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/opencv-env/lib/python3.13/site-packages/torchvision/transforms/transforms.py:973\u001b[39m, in \u001b[36mRandomResizedCrop.forward\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m    965\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    966\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m    967\u001b[39m \u001b[33;03m    img (PIL Image or Tensor): Image to be cropped and resized.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    970\u001b[39m \u001b[33;03m    PIL Image or Tensor: Randomly cropped and resized image.\u001b[39;00m\n\u001b[32m    971\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    972\u001b[39m i, j, h, w = \u001b[38;5;28mself\u001b[39m.get_params(img, \u001b[38;5;28mself\u001b[39m.scale, \u001b[38;5;28mself\u001b[39m.ratio)\n\u001b[32m--> \u001b[39m\u001b[32m973\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresized_crop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mantialias\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/opencv-env/lib/python3.13/site-packages/torchvision/transforms/functional.py:649\u001b[39m, in \u001b[36mresized_crop\u001b[39m\u001b[34m(img, top, left, height, width, size, interpolation, antialias)\u001b[39m\n\u001b[32m    647\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.jit.is_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.jit.is_tracing():\n\u001b[32m    648\u001b[39m     _log_api_usage_once(resized_crop)\n\u001b[32m--> \u001b[39m\u001b[32m649\u001b[39m img = \u001b[43mcrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    650\u001b[39m img = resize(img, size, interpolation, antialias=antialias)\n\u001b[32m    651\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/opencv-env/lib/python3.13/site-packages/torchvision/transforms/functional.py:551\u001b[39m, in \u001b[36mcrop\u001b[39m\u001b[34m(img, top, left, height, width)\u001b[39m\n\u001b[32m    549\u001b[39m     _log_api_usage_once(crop)\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch.Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m F_t.crop(img, top, left, height, width)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/opencv-env/lib/python3.13/site-packages/torchvision/transforms/_functional_pil.py:238\u001b[39m, in \u001b[36mcrop\u001b[39m\u001b[34m(img, top, left, height, width)\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_pil_image(img):\n\u001b[32m    236\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mimg should be PIL Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/opencv-env/lib/python3.13/site-packages/PIL/Image.py:1304\u001b[39m, in \u001b[36mImage.crop\u001b[39m\u001b[34m(self, box)\u001b[39m\n\u001b[32m   1301\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1303\u001b[39m \u001b[38;5;28mself\u001b[39m.load()\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_crop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/opencv-env/lib/python3.13/site-packages/PIL/Image.py:1326\u001b[39m, in \u001b[36mImage._crop\u001b[39m\u001b[34m(self, im, box)\u001b[39m\n\u001b[32m   1322\u001b[39m absolute_values = (\u001b[38;5;28mabs\u001b[39m(x1 - x0), \u001b[38;5;28mabs\u001b[39m(y1 - y0))\n\u001b[32m   1324\u001b[39m _decompression_bomb_check(absolute_values)\n\u001b[32m-> \u001b[39m\u001b[32m1326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train the model.\n",
    "num_epochs = 25\n",
    "trained_model, history, best_epoch = train_and_validate(resnet50, loss_func, optimizer, num_epochs)\n",
    "\n",
    "torch.save(history, dataset+'_history.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed857d24",
   "metadata": {},
   "source": [
    "5.1. Plotting the Training Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c14f4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "history = np.array(history)\n",
    "plt.plot(history[:,0:2])\n",
    "plt.legend(['Training Loss', 'Validation Loss'])\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig('loss_curve.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042d5435",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(history[:,2:4])\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'])\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig('accuracy_curve.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a38c195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTestSetAccuracy(model, loss_criterion):\n",
    "    \"\"\"\n",
    "    Computes the accuracy and loss of the model on the test dataset.\n",
    "\n",
    "    Parameters:\n",
    "    model (torch.nn.Module): The trained model to evaluate.\n",
    "    loss_criterion (torch.nn.Module): The loss function used for evaluation.\n",
    "\n",
    "    The function runs inference on the test dataset without tracking gradients,\n",
    "    calculates the loss and accuracy for each batch, and returns the average loss and accuracy.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    test_acc = 0.0\n",
    "    test_loss = 0.0\n",
    "\n",
    "    # Validation - No gradient tracking needed\n",
    "    with torch.no_grad():\n",
    "        # Set to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Validation loop\n",
    "        for j, (inputs, labels) in enumerate(test_data_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass - compute outputs on input data using the model\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_criterion(outputs, labels)\n",
    "\n",
    "            # Compute the total loss for the batch and add it to test_loss\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Calculate test accuracy\n",
    "            _, predictions = torch.max(outputs.data, 1)\n",
    "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "\n",
    "            # Convert correct_counts to float and then compute the mean\n",
    "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "            # Compute total accuracy in the whole batch and add to test_acc\n",
    "            test_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "            print(f\"Test Batch number: {j:03d}, Test: Loss: {loss.item():.4f}, Accuracy: {acc.item():.4f}\")\n",
    "\n",
    "    # Find average test loss and test accuracy\n",
    "    avg_test_loss = test_loss / test_data_size\n",
    "    avg_test_acc = test_acc / test_data_size\n",
    "\n",
    "    print(\"Test accuracy: {:.4f}\".format(avg_test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbf1282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best saved model during training.\n",
    "model = torch.load(\"best_model.pt\".format(dataset, best_epoch), weights_only=False)\n",
    "# Evaluate the model's performance on the test dataset and print the results.\n",
    "computeTestSetAccuracy(model, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f397d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def prediction(model):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    all_images, all_labels = [], []\n",
    "    all_pred_indices, all_pred_probs = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            prob = F.softmax(outputs, dim=1)\n",
    "            pred_indices = prob.data.max(dim=1)[1]\n",
    "            pred_probs = prob.data.max(dim=1)[0]\n",
    "\n",
    "            all_images.append(images.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "            all_pred_indices.append(pred_indices.cpu())\n",
    "            all_pred_probs.append(pred_probs.cpu())\n",
    "    \n",
    "    return (torch.cat(all_images).numpy(),\n",
    "            torch.cat(all_labels).numpy(),\n",
    "            torch.cat(all_pred_indices).numpy(),\n",
    "            torch.cat(all_pred_probs).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aa67e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "val_images, val_gt_labels, pred_indices, pred_probs = prediction(model, test_data_loader)\n",
    "cm = confusion_matrix(y_true=val_gt_labels, y_pred = pred_indices)\n",
    "\n",
    "plt.figure(figsize= [10,5])\n",
    "sn.heatmap(cm, annot=True, fmt='d', annot_kws={\"size\":14})\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Targets\")\n",
    "plt.title(f\"Confusion Matrix\", color=\"gray\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
